# Conversation Memory Feature: Implementation Plan

This document outlines the plan to implement a conversation memory feature. The goal is to periodically summarize ended conversations to create a persistent, structured memory that the AI can potentially leverage in the future.

## 1. Guiding Principles & Modifications

Based on our discussion, the implementation will follow these principles:

*   **Manual Trigger**: Memory generation will be initiated by a manual API call, not an automated scheduler for now.
*   **Batch Processing**: To improve efficiency, the backend will find all eligible conversations and enqueue them for processing in a single SQS message.
*   **Reuse Existing Infrastructure**: The feature will be built into the existing `Backend` and `Brain` services, without creating new Lambdas or infrastructure components.
*   **Local-First Development**: All infrastructure changes (Terraform, IAM) are deferred. The focus is on code changes that can be tested locally.
*   **Model-First Migrations**: We will update the SQLAlchemy models directly. Database migration scripts will be auto-generated by you later.

---

## 2. Architecture & Data Flow

The process is initiated by a manual call and uses a batch SQS message to communicate between the Backend and Brain.

```mermaid
sequenceDiagram
    participant User as "Developer (Manual Trigger)"
    participant Backend
    participant SQS
    participant Brain
    participant DB as "PostgreSQL DB"
    participant LLM

    User->>+Backend: POST /tasks/trigger-memory-generation
    Note over Backend: Finds all conversations needing memory<br>update based on an inactivity threshold.
    
    Backend->>+DB: Find conversations to process
    DB-->>-Backend: List of conversation IDs
    
    Note over Backend: Group all IDs into one message.
    Backend->>+SQS: Enqueue { "task_type": "GENERATE_MEMORY_BATCH", "conversation_ids": [...] }
    SQS-->>-Backend: 
    
    SQS-->>+Brain: Consume SQS batch message
    
    alt Is a "GENERATE_MEMORY_BATCH" task
        Note over Brain: Loop through each ID in the batch.
        loop For each conversation_id in message
            Brain->>+Backend: GET /conversations/{id}/messages
            Backend-->>-Brain: Full conversation history
            
            Brain->>+LLM: Generate summary from history
            LLM-->>-Brain: JSON-structured memory
            
            Brain->>+Backend: POST /memories (to UPSERT memory)
            Backend->>+DB: INSERT or UPDATE memory
            DB-->>-Backend: 
            Backend-->>-Brain: Success
        end
    else Handle regular message
        Note right of Brain: Existing message flow
    end
    
    Brain-->>-SQS: Acknowledge Message
```

---

## 3. Database Schema Changes

We will add a `conversation_memories` table to store the structured output from the LLM.

```mermaid
erDiagram
    users {
        int id PK
        string phone_number "unique"
    }
    conversations {
        int id PK
        int user_id FK
        datetime updated_at
    }
    conversation_memories {
        int id PK
        int conversation_id FK "unique"
        jsonb memory_data
        datetime created_at
        datetime updated_at
    }
    messages {
        int id PK
        int conversation_id FK
    }

    users ||--o{ conversations : "has"
    conversations ||--o{ messages : "contains"
    conversations |o--|| conversation_memories : "has one"
}
```
*   **`conversation_memories` Table**:
    *   `id`: Primary Key.
    *   `conversation_id`: A unique foreign key to the `conversations` table, ensuring a one-to-one relationship.
    *   `memory_data`: A `JSONB` column to store the structured memory from the LLM.
    *   `created_at`, `updated_at`: Timestamps for tracking.

---

## 4. Step-by-Step Implementation

### A. Backend Service Changes

#### 1. Define New SQLAlchemy Model
In your models file (e.g., `backend/app/models.py`), add the model for the new table.

```python
# In your models file...
from sqlalchemy import Column, Integer, ForeignKey, JSON
from sqlalchemy.orm import relationship
from .database import Base # Assuming you have a Base declarative

class ConversationMemory(Base):
    __tablename__ = "conversation_memories"

    id = Column(Integer, primary_key=True, index=True)
    conversation_id = Column(Integer, ForeignKey("conversations.id"), unique=True, nullable=False, index=True)
    memory_data = Column(JSON, nullable=False)
    
    # Add created_at, updated_at as needed
    
    conversation = relationship("Conversation", back_populates="memory")

# In your Conversation model, add the corresponding relationship:
class Conversation(Base):
    # ... existing columns
    memory = relationship("ConversationMemory", back_populates="conversation", uselist=False, cascade="all, delete-orphan")
```

#### 2. Define New Pydantic Schemas
In your schemas file (e.g., `backend/app/schemas.py`), define the schemas for API data validation.

```python
# In your schemas file...
from pydantic import BaseModel
from typing import Any

class MemoryBase(BaseModel):
    memory_data: dict[str, Any]

class MemoryCreate(MemoryBase):
    conversation_id: int

class MemoryUpdate(MemoryBase):
    pass

class MemoryInDB(MemoryBase):
    id: int
    conversation_id: int
    
    class Config:
        orm_mode = True
```

#### 3. Create New API Endpoints

**Endpoint 1: Trigger Memory Generation (Manual)**

This endpoint finds eligible conversations and sends them to the Brain in a single batch.

*   **File**: e.g., `backend/app/api/endpoints/tasks.py`
*   **Route**: `POST /tasks/trigger-memory-generation`
*   **Logic**:
    1.  Define an `INACTIVITY_THRESHOLD` (e.g., 2 hours). Make it configurable via an environment variable.
    2.  Query the DB to find all conversations updated before the threshold that either don't have a memory or have new messages since the memory was last updated.
    3.  Collect all `conversation_id`s from the query result.
    4.  If IDs are found, construct a single SQS message `{ "task_type": "GENERATE_MEMORY_BATCH", "conversation_ids": [...] }`.
    5.  Send the message to the SQS queue.

**Endpoint 2: Create/Update a Memory**

This endpoint is called by the Brain to save a generated memory.

*   **File**: e.g., `backend/app/api/endpoints/memories.py`
*   **Route**: `POST /memories`
*   **Logic**:
    1.  Accept a payload containing `conversation_id` and `memory_data`.
    2.  Perform an "UPSERT":
        *   Try to find an existing `ConversationMemory` for the `conversation_id`.
        *   If it exists, `UPDATE` its `memory_data` and `updated_at` timestamp.
        *   If it doesn't exist, `INSERT` a new `ConversationMemory` record.
    3.  Return the created/updated memory object.

### B. Brain Service Changes

#### 1. Update SQS Consumer Logic

Modify the main SQS handler to recognize the new batch task type.

*   **File**: Your main SQS consumer logic in the `Brain` service.
*   **Logic**:
    1.  Check the incoming message for `task_type`.
    2.  If `task_type == "GENERATE_MEMORY_BATCH"`:
        a. Extract the list of `conversation_ids`.
        b. **Loop through each `conversation_id`** in the list.
        c. **Important**: Wrap the processing for each ID in a `try...except` block. If one conversation fails (e.g., LLM error), log the failure and continue to the next ID. This makes the batch processing resilient.
        d. Inside the loop, for each ID:
            i. Call the Backend API to get the full message history for the conversation.
            ii. Construct the prompt for the LLM, including the conversation history and instructions for the JSON output format.
            iii. Call the LLM to get the structured JSON memory.
            iv. Call the Backend's `POST /memories` endpoint to save the result.
    3.  `else`: Handle the regular message processing as it currently does.

This plan provides a clear path forward for developing the feature locally. Once this is working, we can then proceed with deploying and setting up the automated trigger. 